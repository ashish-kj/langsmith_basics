{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee65711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OPENAI_API_KEY configured\n",
      "✓ LANGCHAIN_API_KEY configured\n",
      "\n",
      "LangSmith Project: langsmith-tutorial-demo\n",
      "\n",
      "Tracing is now active - all AI operations will be logged for analysis\n",
      "Visit https://smith.langchain.com to see your traces\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configure API keys - replace with your actual keys\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'  # This triggers observability\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'langsmith-tutorial-demo' #This is the project name where the traces will be stored\n",
    "\n",
    "# Verify configuration\n",
    "required_vars = ['OPENAI_API_KEY', 'LANGCHAIN_API_KEY']\n",
    "for var in required_vars:\n",
    "    if not os.getenv(var) or 'your_' in os.getenv(var, ''):\n",
    "        print(f\"Warning: {var} needs your actual key\")\n",
    "    else:\n",
    "        print(f\"✓ {var} configured\")\n",
    "\n",
    "print(f\"\\nLangSmith Project: {os.getenv('LANGCHAIN_PROJECT')}\")\n",
    "print(\"\\nTracing is now active - all AI operations will be logged for analysis\")\n",
    "print(\"Visit https://smith.langchain.com to see your traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bad6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized with temperature=0 for consistent behavior\n",
      "All LLM calls will be automatically traced in LangSmith\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize the language model with deterministic settings\n",
    "# Using temperature=0 ensures consistent responses, making it easier to analyze patterns\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Language model initialized with temperature=0 for consistent behavior\")\n",
    "print(\"All LLM calls will be automatically traced in LangSmith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b936d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state schema defined\n",
      "This structured state enables LangSmith to track information flow\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Simple state that flows through our agent workflow.\"\"\"\n",
    "    user_question: str        # The original question from the user\n",
    "    needs_search: bool        # Whether we determined search is needed\n",
    "    search_result: str        # Result from our search tool (if used)\n",
    "    final_answer: str         # The response we'll give to the user\n",
    "    reasoning: str            # Why we made our decisions (great for observability)\n",
    "\n",
    "print(\"Agent state schema defined\")\n",
    "print(\"This structured state enables LangSmith to track information flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ba62bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search tool created with proper Wikipedia search API integration\n",
      "Tool execution timing and results will be captured automatically\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    \"\"\"Search Wikipedia for current information about a topic.\"\"\"\n",
    "    try:\n",
    "        # Use Wikipedia's proper search API that can handle general queries\n",
    "        # This is different from the page summary API which requires exact page titles\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": query,\n",
    "            \"format\": \"json\",\n",
    "            \"srlimit\": 3  # Get top 3 results\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, params=search_params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            search_results = data.get('query', {}).get('search', [])\n",
    "            \n",
    "            if search_results:\n",
    "                # Get the most relevant result and fetch its summary\n",
    "                top_result = search_results[0]\n",
    "                page_title = top_result['title']\n",
    "                \n",
    "                # Now get the page summary using the exact title\n",
    "                summary_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
    "                summary_response = requests.get(summary_url, timeout=10)\n",
    "                \n",
    "                if summary_response.status_code == 200:\n",
    "                    summary_data = summary_response.json()\n",
    "                    extract = summary_data.get('extract', 'No summary available')\n",
    "                    # Truncate for readability in traces\n",
    "                    return f\"Found information about '{page_title}': {extract[:400]}...\"\n",
    "                else:\n",
    "                    return f\"Found '{page_title}' but couldn't retrieve summary\"\n",
    "            else:\n",
    "                return f\"No Wikipedia articles found for '{query}'\"\n",
    "        else:\n",
    "            return f\"Wikipedia search failed with status {response.status_code}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        # This error handling will show up in LangSmith traces\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "print(\"Search tool created with proper Wikipedia search API integration\")\n",
    "print(\"Tool execution timing and results will be captured automatically\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472f5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_search_need(state: AgentState) -> AgentState:\n",
    "    \"\"\"Analyze the question and decide if we need to search for current information.\"\"\"\n",
    "    user_question = state[\"user_question\"]\n",
    "    \n",
    "    # This prompt engineering is visible in LangSmith traces\n",
    "    # Notice how we're asking for a structured response to make parsing reliable\n",
    "    decision_prompt = f\"\"\"\n",
    "    Analyze this question and decide if it requires current/recent information that might not be in your training data:\n",
    "    \n",
    "    Question: \"{user_question}\"\n",
    "    \n",
    "    Consider:\n",
    "    - Does this ask about recent events, current prices, or breaking news?\n",
    "    - Does this ask about people, companies, or topics that change frequently?\n",
    "    - Can you answer this well using your existing knowledge?\n",
    "    \n",
    "    Respond with exactly \"SEARCH\" if you need current information, or \"DIRECT\" if you can answer directly.\n",
    "    Then on a new line, briefly explain your reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([SystemMessage(content=decision_prompt)])\n",
    "    decision_text = response.content.strip()\n",
    "    \n",
    "    # Parse the response - this parsing logic will be visible in traces\n",
    "    lines = decision_text.split('\\n')\n",
    "    decision = lines[0].strip()\n",
    "    reasoning = lines[1] if len(lines) > 1 else \"No reasoning provided\"\n",
    "    \n",
    "    # Update state with our decision\n",
    "    state[\"needs_search\"] = decision == \"SEARCH\"\n",
    "    state[\"reasoning\"] = f\"Decision: {decision}. Reasoning: {reasoning}\"\n",
    "    \n",
    "    # This print statement will help you see the flow during execution\n",
    "    print(f\"Decision: {'SEARCH' if state['needs_search'] else 'DIRECT'} - {reasoning}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d71c22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute search if needed, otherwise skip this step.\"\"\"\n",
    "    if not state[\"needs_search\"]:\n",
    "        print(\"Skipping search - not needed for this question\")\n",
    "        state[\"search_result\"] = \"No search performed\"\n",
    "        return state\n",
    "    \n",
    "    print(f\"Executing search for: {state['user_question']}\")\n",
    "    \n",
    "    # Execute our search tool - this will show up as a separate step in LangSmith\n",
    "    # The .invoke() call will be traced with full input/output details\n",
    "    search_result = wikipedia_search.invoke({\"query\": state[\"user_question\"]})\n",
    "    state[\"search_result\"] = search_result\n",
    "    \n",
    "    print(f\"Search completed: {len(search_result)} characters returned\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6922d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate the final response using all available information.\"\"\"\n",
    "    user_question = state[\"user_question\"]\n",
    "    search_result = state.get(\"search_result\", \"\")\n",
    "    used_search = state[\"needs_search\"]\n",
    "    \n",
    "    # Build context for the response\n",
    "    # This conditional logic creates different prompt patterns that LangSmith will capture\n",
    "    if used_search and \"Search error\" not in search_result:\n",
    "        context = f\"Question: {user_question}\\n\\nSearch Results: {search_result}\"\n",
    "        response_prompt = f\"\"\"\n",
    "        Answer the user's question using both your knowledge and the search results provided.\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Provide a helpful, accurate response that synthesizes the information.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        response_prompt = f\"\"\"\n",
    "        Answer this question using your existing knowledge:\n",
    "        \n",
    "        {user_question}\n",
    "        \n",
    "        Provide a helpful, accurate response.\n",
    "        \"\"\"\n",
    "    \n",
    "    # This LLM call will be traced with the complete prompt and response\n",
    "    response = llm.invoke([SystemMessage(content=response_prompt)])\n",
    "    state[\"final_answer\"] = response.content\n",
    "    \n",
    "    print(f\"Response generated: {len(response.content)} characters\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89b0a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow compiled successfully\n",
      "Flow: decide → search → generate_response\n",
      "Ready to demonstrate LangSmith observability\n"
     ]
    }
   ],
   "source": [
    "# Build the workflow graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add our three workflow steps\n",
    "# Each of these will appear as a distinct node in LangSmith's graph visualization\n",
    "workflow.add_node(\"decide\", decide_search_need)\n",
    "workflow.add_node(\"search\", execute_search)\n",
    "workflow.add_node(\"respond\", generate_response)\n",
    "\n",
    "# Define the flow with conditional logic\n",
    "# LangSmith will show you which edges were traversed for each execution\n",
    "workflow.set_entry_point(\"decide\")\n",
    "workflow.add_edge(\"decide\", \"search\")     # Always go to search step (it will skip if not needed)\n",
    "workflow.add_edge(\"search\", \"respond\")    # Then generate response\n",
    "workflow.add_edge(\"respond\", END)         # Finish\n",
    "\n",
    "# Compile into an executable agent\n",
    "simple_agent = workflow.compile()\n",
    "\n",
    "print(\"Workflow compiled successfully\")\n",
    "print(\"Flow: decide → search → generate_response\")\n",
    "print(\"Ready to demonstrate LangSmith observability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "357becf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_with_observability(question: str, test_type: str) -> dict:\n",
    "    \"\"\"Run a test and capture comprehensive observability data.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {question}\")\n",
    "    print(f\"Type: {test_type}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize state for this test\n",
    "    initial_state = {\n",
    "        \"user_question\": question,\n",
    "        \"needs_search\": False,\n",
    "        \"search_result\": \"\",\n",
    "        \"final_answer\": \"\",\n",
    "        \"reasoning\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Execute with metadata for LangSmith\n",
    "        # This metadata will help you filter and analyze traces later\n",
    "        config = {\n",
    "            \"metadata\": {\n",
    "                \"test_type\": test_type,\n",
    "                \"tutorial\": \"langsmith-observability\"\n",
    "            },\n",
    "            \"tags\": [\"tutorial\", \"demo\", test_type]\n",
    "        }\n",
    "        \n",
    "        # This invoke call will create a complete trace in LangSmith\n",
    "        final_state = simple_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Display results for immediate feedback\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"   Decision Process: {final_state['reasoning']}\")\n",
    "        print(f\"   Used Search: {'Yes' if final_state['needs_search'] else 'No'}\")\n",
    "        print(f\"   Response Length: {len(final_state['final_answer'])} characters\")\n",
    "        print(f\"   Total Time: {total_time:.2f} seconds\")\n",
    "        print(f\"\\nAnswer: {final_state['final_answer'][:200]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"type\": test_type,\n",
    "            \"success\": True,\n",
    "            \"used_search\": final_state['needs_search'],\n",
    "            \"total_time\": round(total_time, 2),\n",
    "            \"reasoning\": final_state['reasoning']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"type\": test_type,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cc35179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LangSmith Observability Demo\n",
      "Each test will generate detailed traces in your LangSmith dashboard\n",
      "Visit https://smith.langchain.com to see real-time traces\n",
      "\n",
      "\n",
      "Running Test 1/3\n",
      "\n",
      "============================================================\n",
      "Testing: What is the capital of France?\n",
      "Type: direct_answer\n",
      "============================================================\n",
      "Decision: DIRECT - The capital of France is Paris, which is a well-established fact that does not change over time.\n",
      "Skipping search - not needed for this question\n",
      "Response generated: 31 characters\n",
      "\n",
      "Results:\n",
      "   Decision Process: Decision: DIRECT. Reasoning: The capital of France is Paris, which is a well-established fact that does not change over time.\n",
      "   Used Search: No\n",
      "   Response Length: 31 characters\n",
      "   Total Time: 2.58 seconds\n",
      "\n",
      "Answer: The capital of France is Paris....\n",
      "\n",
      "Running Test 2/3\n",
      "\n",
      "============================================================\n",
      "Testing: What happened in the 2024 US presidential election?\n",
      "Type: current_info\n",
      "============================================================\n",
      "Decision: SEARCH - \n",
      "Executing search for: What happened in the 2024 US presidential election?\n",
      "Search completed: 448 characters returned\n",
      "Response generated: 518 characters\n",
      "\n",
      "Results:\n",
      "   Decision Process: Decision: SEARCH. Reasoning: \n",
      "   Used Search: Yes\n",
      "   Response Length: 518 characters\n",
      "   Total Time: 3.50 seconds\n",
      "\n",
      "Answer: In the 2024 United States presidential election, held on November 5, 2024, the Republican Party's ticket, consisting of former President Donald Trump and junior U.S. Senator JD Vance from Ohio, emerge...\n",
      "\n",
      "Running Test 3/3\n",
      "\n",
      "============================================================\n",
      "Testing: Tell me about artificial intelligence\n",
      "Type: factual_lookup\n",
      "============================================================\n",
      "Decision: DIRECT - The question is broad and does not specifically ask for recent events or current developments in artificial intelligence. I can provide a comprehensive overview based on my existing knowledge.\n",
      "Skipping search - not needed for this question\n",
      "Response generated: 2518 characters\n",
      "\n",
      "Results:\n",
      "   Decision Process: Decision: DIRECT. Reasoning: The question is broad and does not specifically ask for recent events or current developments in artificial intelligence. I can provide a comprehensive overview based on my existing knowledge.\n",
      "   Used Search: No\n",
      "   Response Length: 2518 characters\n",
      "   Total Time: 6.47 seconds\n",
      "\n",
      "Answer: Artificial intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning (the acquisition of information and r...\n",
      "\n",
      "\n",
      "All tests completed\n",
      "Generated 3 traces in LangSmith\n",
      "Check your dashboard to explore the detailed execution data\n"
     ]
    }
   ],
   "source": [
    "# Define our test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"type\": \"direct_answer\",\n",
    "        \"expected_search\": False\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What happened in the 2024 US presidential election?\",\n",
    "        \"type\": \"current_info\",\n",
    "        \"expected_search\": True\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Tell me about artificial intelligence\",\n",
    "        \"type\": \"factual_lookup\",\n",
    "        \"expected_search\": False  # Should be answerable directly\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Starting LangSmith Observability Demo\")\n",
    "print(\"Each test will generate detailed traces in your LangSmith dashboard\")\n",
    "print(\"Visit https://smith.langchain.com to see real-time traces\\n\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nRunning Test {i}/{len(test_cases)}\")\n",
    "    \n",
    "    result = run_test_with_observability(\n",
    "        test_case[\"question\"], \n",
    "        test_case[\"type\"]\n",
    "    )\n",
    "    \n",
    "    test_results.append(result)\n",
    "    \n",
    "    # Small delay to make the traces easier to distinguish in LangSmith\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n\\nAll tests completed\")\n",
    "print(f\"Generated {len(test_results)} traces in LangSmith\")\n",
    "print(f\"Check your dashboard to explore the detailed execution data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
